{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.face_utils import FaceAligner\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import shutil\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Activation\n",
    "from keras.layers import Conv2D, AveragePooling2D\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224)\n",
    "# yuz tespiti yapan detektor\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# fotografta yuz var mi kontrolu\n",
    "def check_detection(img):\n",
    "    exact_image = False\n",
    "    if type(img).__module__ == np.__name__:\n",
    "        exact_image = True\n",
    "\n",
    "    base64_img = False\n",
    "    if len(img) > 11 and img[0:11] == \"data:image/\":\n",
    "        base64_img = True\n",
    "\n",
    "\n",
    "    elif not exact_image:  # image path passed as input\n",
    "\n",
    "        if not os.path.isfile(img):\n",
    "            raise ValueError(\"Confirm that \", img, \" exists\")\n",
    "\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    rects, scores, idx = face_detector.run(gray,1,-1)\n",
    "            \n",
    "    if len(rects) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# yuz tespit eden fonksyion\n",
    "def detect_face(img):\n",
    "    exact_image = False\n",
    "    if type(img).__module__ == np.__name__:\n",
    "        exact_image = True\n",
    "\n",
    "    base64_img = False\n",
    "    if len(img) > 11 and img[0:11] == \"data:image/\":\n",
    "        base64_img = True\n",
    "\n",
    "    elif not exact_image:  # image path passed as input\n",
    "        if not os.path.isfile(img):\n",
    "            raise ValueError(\"Confirm that \", img, \" exists\")\n",
    "\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # rects: yuz tespiti yapilan cerceve\n",
    "    # scores: yuz tespiti skorlari\n",
    "    # best_score_index: en yuksek skora sahip yuzun indisi\n",
    "    rects, scores, idx = face_detector.run(gray,1,-1)\n",
    "    best_score_index = np.argmax(scores)\n",
    "    faces = []\n",
    "    # tespit edilen yuzler fotograftan kirpiliyor \n",
    "    for rect in rects:\n",
    "        detected_face = img[max(0, rect.top()): min(rect.bottom(), img.shape[0]),\n",
    "                    max(0, rect.left()): min(rect.right(), img.shape[1])]\n",
    "        \n",
    "        # fotograflar 224, 224 hale getiriliyor\n",
    "        detected_face = cv2.resize(detected_face, (target_size[0], target_size[1]))\n",
    "        faces.append(detected_face)\n",
    "\n",
    "    return faces[best_score_index]\n",
    "\n",
    "# path'deki dosyalarin okunmasi icin gerekli\n",
    "def absoluteFilePaths(directory):\n",
    "    paths_image_list = []\n",
    "    for dirpath,_,filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "            paths_image_list.append(os.path.abspath(os.path.join(dirpath, f)))\n",
    "    return paths_image_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['East Asian', 'White', 'Latino_Hispanic', 'Southeast Asian',\n",
       "       'Black', 'Indian', 'Middle Eastern'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fotojenik olmayanlara ait bilgiler okunuyor\n",
    "normals_df = pd.read_csv(\"fairface/fairface_label_val.csv\")\n",
    "races = normals_df['race'].unique()\n",
    "races"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hangi kombinasyon icin model egitilecekse, ilgili path ve conditionlar duzenlenmeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hangi etnik koken ve cinsiyet icin egitim yapilacaksa ona gore ilgili path duzenlenmeli\n",
    "paths = absoluteFilePaths(\"C:/Bitirme/beauty_dataset/White/men/contestants\")\n",
    "\n",
    "df = pd.DataFrame(paths, columns = ['Path'])\n",
    "df['beauty'] = 1\n",
    "df = df.rename(columns={'Path': 'file'})\n",
    "\n",
    "# hangi etnik koken icin egitim yapilacaksa filtrelenmek icin == karsiligina yazilmali\n",
    "# etnik koken etiketleri : \n",
    "# ['East Asian', 'White', 'Latino_Hispanic', 'Southeast Asian', 'Black', 'Indian', 'Middle Eastern']\n",
    "normals_df = normals_df[normals_df['race'] == 'White']\n",
    "\n",
    "# hangi cinsiyet icin egitim yapilacaksa belirtilmeli: [Male, Female]\n",
    "normals_df = normals_df[normals_df['gender'] == 'Male']\n",
    "\n",
    "normals_df = normals_df.drop(columns=['age', 'service_test'])\n",
    "normals_df = normals_df.head(len(df)*5)\n",
    "normals_df['beauty'] = 0\n",
    "\n",
    "# fotojenik olmayanlarin fotograflarinin bulundugu fairface dataseti pathi verilmeli \n",
    "normals_df['file'] = normals_df['file'].apply(lambda x: 'C:/Bitirme/fairface/'+x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yuz tespiti yapilamayan fotograflar eleniyor\n",
    "df['detection'] = df['file'].apply(check_detection)\n",
    "df = df[df.detection == True]\n",
    "df = df.drop(columns='detection')\n",
    "\n",
    "normals_df['detection'] = normals_df['file'].apply(check_detection)\n",
    "normals_df = normals_df[normals_df.detection == True]\n",
    "normals_df = normals_df.drop(columns=['detection', 'gender', 'race'])\n",
    "normals_df = normals_df.head(len(df))\n",
    "\n",
    "# fotojenik ve fotojenik olmayanlara ait veriler tek bir dataframe de birlestiriliyor\n",
    "union_df = pd.concat([df, normals_df]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                file  beauty\n",
      "0  C:\\Bitirme\\beauty_dataset\\White\\men\\contestant...       1\n",
      "1  C:\\Bitirme\\beauty_dataset\\White\\men\\contestant...       1\n",
      "2  C:\\Bitirme\\beauty_dataset\\White\\men\\contestant...       1\n",
      "3  C:\\Bitirme\\beauty_dataset\\White\\men\\contestant...       1\n",
      "4  C:\\Bitirme\\beauty_dataset\\White\\men\\contestant...       1\n",
      "                              file  beauty\n",
      "2    C:/Bitirme/fairface/val/3.jpg       0\n",
      "18  C:/Bitirme/fairface/val/19.jpg       0\n",
      "21  C:/Bitirme/fairface/val/22.jpg       0\n",
      "44  C:/Bitirme/fairface/val/45.jpg       0\n",
      "46  C:/Bitirme/fairface/val/47.jpg       0\n",
      "                                                file  beauty\n",
      "0  C:\\Bitirme\\beauty_dataset\\White\\men\\contestant...       1\n",
      "1  C:\\Bitirme\\beauty_dataset\\White\\men\\contestant...       1\n",
      "2  C:\\Bitirme\\beauty_dataset\\White\\men\\contestant...       1\n",
      "3  C:\\Bitirme\\beauty_dataset\\White\\men\\contestant...       1\n",
      "4  C:\\Bitirme\\beauty_dataset\\White\\men\\contestant...       1\n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))\n",
    "print(normals_df.head(5))\n",
    "print(union_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veriler uygun sekle getirilerek train, test olarak ayriliyor\n",
    "classes = 2\n",
    "target = union_df['beauty'].values\n",
    "target_classes = keras.utils.to_categorical(target, classes)\n",
    "\n",
    "features = []\n",
    "\n",
    "for i in range(0, union_df.shape[0]):\n",
    "    features.append(union_df['file'].values[i])\n",
    "    \n",
    "train_x, test_x, train_y, test_y = train_test_split(features, target_classes, test_size=0.20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yoruma alinmis kisimlar, yuz tespiti yaparak kirpilmis yuzleri fotograf olarak kaydediyor, istenilirse acilabilir. Fotograflarin kaydedilmesine gerek yoksa bu kisim atlanabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = 'C:/Bitirme/beauty_dataset/White/men/'\n",
    "# train_path = path + 'train/'\n",
    "# test_path = path + 'test/'\n",
    "# normals_path = path + 'normals/'\n",
    "\n",
    "# if not os.path.exists(normals_path):\n",
    "#     os.mkdir(normals_path)\n",
    "    \n",
    "#     for img_path in normals_df['file'].tolist():\n",
    "#         img = cv2.imread(img_path)\n",
    "#         img_name = img_path.split('/')[-1]\n",
    "#         cv2.imwrite(normals_path+img_name, img)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if os.path.exists(train_path) and os.path.exists(test_path):\n",
    "    # bu kisma dikkat, yanlis bi klasor silinmemesi icin train_path, test_path dogru verilmeli\n",
    "#     shutil.rmtree(train_path)\n",
    "#     shutil.rmtree(test_path)\n",
    "\n",
    "# os.mkdir(path+'train')\n",
    "# os.mkdir(path+'test')\n",
    "\n",
    "# temp_list = []\n",
    "# for i in train_x:\n",
    "#     if '/' in i:\n",
    "#         img_name = i.split('/')[-1]\n",
    "#     else:\n",
    "#         img_name = i.split('\\\\')[-1]\n",
    "#     img = detect_face(i)\n",
    "#     temp_list.append(img)\n",
    "#     #img = cv2.imread(i)\n",
    "#     #cv2.imwrite(train_path+img_name, img)\n",
    "\n",
    "# train_x = temp_list\n",
    "# temp_list2 = []\n",
    "    \n",
    "# for i in test_x:\n",
    "#     if '/' in i:\n",
    "#         img_name = i.split('/')[-1]\n",
    "#     else:\n",
    "#         img_name = i.split('\\\\')[-1]\n",
    "#     img = detect_face(i)\n",
    "#     temp_list2.append(img)\n",
    "#     #img = cv2.imread(i)\n",
    "#     #cv2.imwrite(test_path+img_name, img)\n",
    "\n",
    "# test_x = temp_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x = np.array(train_x)/255\n",
    "# test_x = np.array(test_x)/255\n",
    "\n",
    "# train_x, val_x, train_y, val_y = train_test_split(train_x, train_y\n",
    "#                                         , test_size=0.1, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg-face modeli\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1, 1), input_shape=(224, 224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.load_weights('vgg_face_weights.h5')\n",
    "\n",
    "for layer in model.layers[:-7]:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fogograflarda yuz tespiti yapiliyor ve uygun egitim icin hale getiriliyor\n",
    "img_pixels = [detect_face(x) for x in features]\n",
    "img_pixels = np.array(img_pixels)/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#del beauty_model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " fold:1\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 2s 630ms/step - loss: 0.6145 - accuracy: 0.7931\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 212ms/step - loss: 0.9689 - accuracy: 0.6897\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 1s 235ms/step - loss: 0.1141 - accuracy: 0.9540\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 1s 252ms/step - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0265 - accuracy: 0.9770\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 1.1252e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 1s 212ms/step - loss: 2.7312e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 1s 212ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 1s 214ms/step - loss: 8.9285e-06 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 214ms/step - loss: 1.7096e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9231\n",
      "\n",
      " fold:2\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 213ms/step - loss: 0.3174 - accuracy: 0.8736\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 231ms/step - loss: 3.9377e-04 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 0.0217 - accuracy: 0.9885\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 1.3924e-05 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 9.0738e-04 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 1.5198e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 1.3688e-06 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 2.7404e-09 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 0.0152 - accuracy: 0.9885\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 238ms/step - loss: 1.2304e-06 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 992us/step - loss: 5.1231e-04 - accuracy: 1.0000\n",
      "\n",
      " fold:3\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 1.4922 - accuracy: 0.6552\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 229ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 6.1301e-04 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 2.7342e-04 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 3.0472e-05 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 1s 224ms/step - loss: 2.8363e-07 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 1s 223ms/step - loss: 2.5971e-05 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 1s 226ms/step - loss: 6.6605e-05 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 1s 225ms/step - loss: 2.6034e-07 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 221ms/step - loss: 1.7068e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.5956e-06 - accuracy: 1.0000\n",
      "\n",
      " fold:4\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 249ms/step - loss: 0.4858 - accuracy: 0.7816\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 252ms/step - loss: 0.0252 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 2.0416e-05 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 1s 240ms/step - loss: 8.3583e-08 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 1s 214ms/step - loss: 4.5217e-08 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 1s 215ms/step - loss: 1.3236e-06 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 1s 228ms/step - loss: 1.6580e-07 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 4.4181e-06 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 0.0442 - accuracy: 0.9885\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 1.8361e-07 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "\n",
      " fold:5\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 2s 783ms/step - loss: 0.7054 - accuracy: 0.7841\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 247ms/step - loss: 0.0168 - accuracy: 0.9886\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 1s 249ms/step - loss: 2.3977e-07 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 6.0551e-07 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 2.0360e-05 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 1s 260ms/step - loss: 0.0281 - accuracy: 0.9886\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 1s 255ms/step - loss: 3.7253e-07 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 1s 251ms/step - loss: 2.1392e-05 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 275ms/step - loss: 1.3817e-07 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8743e-07 - accuracy: 1.0000\n",
      "\n",
      " fold:6\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 218ms/step - loss: 2.1908 - accuracy: 0.5682\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 212ms/step - loss: 0.0110 - accuracy: 0.9886\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 1s 210ms/step - loss: 0.0831 - accuracy: 0.9773\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 1s 212ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 7.3816e-06 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 1s 284ms/step - loss: 2.1077e-06 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 1s 248ms/step - loss: 6.9087e-08 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 1s 263ms/step - loss: 3.6576e-08 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 1s 270ms/step - loss: 2.2309e-06 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 251ms/step - loss: 5.1060e-06 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 992us/step - loss: 2.9802e-08 - accuracy: 1.0000\n",
      "\n",
      " fold:7\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 264ms/step - loss: 0.1848 - accuracy: 0.8864\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 254ms/step - loss: 1.2899e-04 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 1s 281ms/step - loss: 4.9444e-07 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 1s 267ms/step - loss: 2.3029e-08 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 2.7093e-09 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 1s 280ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 1s 244ms/step - loss: 3.9292e-06 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 1s 282ms/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 5.4186e-09 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 276ms/step - loss: 2.7093e-09 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 965us/step - loss: 0.0000e+00 - accuracy: 1.0000\n",
      "\n",
      " fold:8\n",
      "Epoch 1/10\n",
      "3/3 [==============================] - 1s 270ms/step - loss: 0.6401 - accuracy: 0.8523\n",
      "Epoch 2/10\n",
      "3/3 [==============================] - 1s 237ms/step - loss: 1.7983e-04 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "3/3 [==============================] - 1s 266ms/step - loss: 2.3029e-08 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "3/3 [==============================] - 1s 253ms/step - loss: 3.0426e-04 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "3/3 [==============================] - 1s 262ms/step - loss: 8.3633e-05 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "3/3 [==============================] - 1s 269ms/step - loss: 5.0491e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "3/3 [==============================] - 1s 243ms/step - loss: 6.3077e-06 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "3/3 [==============================] - 1s 276ms/step - loss: 2.6776e-05 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "3/3 [==============================] - 1s 252ms/step - loss: 4.6599e-07 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "3/3 [==============================] - 1s 256ms/step - loss: 2.2432e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 496us/step - loss: 0.0000e+00 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "acc_per_fold = []\n",
    "loss_per_fold = []\n",
    "cm_per_fold = []\n",
    "\n",
    "kfold = KFold(n_splits=8, shuffle=True)\n",
    "fold_index = 1\n",
    "for train, test in kfold.split(features, target_classes):\n",
    "    print(f'\\n fold:{fold_index}')\n",
    "    fold_index += 1\n",
    "    \n",
    "    # model bastan olusturuluyor\n",
    "    base_model_output = Sequential()\n",
    "    base_model_output = Convolution2D(classes, (1, 1), name='predictions')(model.layers[-4].output)\n",
    "    base_model_output = Flatten()(base_model_output)\n",
    "    base_model_output = Activation('softmax')(base_model_output)\n",
    "\n",
    "    beauty_model = Model(inputs=model.input, outputs=base_model_output)\n",
    "\n",
    "    sgd = keras.optimizers.SGD(lr=1e-3, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "    beauty_model.compile(loss='categorical_crossentropy'\n",
    "                      , optimizer=keras.optimizers.Adam()\n",
    "                      #, optimizer = sgd\n",
    "                      #, optimizer = 'sgd'   \n",
    "                      , metrics=['accuracy']\n",
    "                     )\n",
    "    \n",
    "    epochs = 10\n",
    "\n",
    "    score = beauty_model.fit(\n",
    "            img_pixels[train], target_classes[train],\n",
    "            epochs=epochs\n",
    "        )\n",
    "    \n",
    "    # test islemi\n",
    "    results = beauty_model.evaluate(img_pixels[test], target_classes[test], verbose=1)\n",
    "    \n",
    "    # accuracy ve loss kaydediliyor\n",
    "    acc_per_fold.append(results[1] * 100)\n",
    "    loss_per_fold.append(results[0])\n",
    "    \n",
    "    # confusion matrix olusturuluyor\n",
    "    predictions = beauty_model.predict(img_pixels[test])\n",
    "    test_y = target_classes[test]\n",
    "    prediction_classes = []; actual_classes = []\n",
    "    beauty_classes = ['Beautiful', 'Normal']\n",
    "    for i in range(0, predictions.shape[0]):\n",
    "        prediction = np.argmax(predictions[i])\n",
    "        prediction_classes.append(beauty_classes[prediction])\n",
    "        actual = np.argmax(test_y[i])\n",
    "        actual_classes.append(beauty_classes[actual])\n",
    "\n",
    "\n",
    "    cm = confusion_matrix(actual_classes, prediction_classes)\n",
    "    \n",
    "    # confusion matrix kaydedildi\n",
    "    cm_per_fold.append(cm)\n",
    "\n",
    "    # k fold ile egitim icin her seferinde model bastan olusturuluyor\n",
    "    # alt satirdaki 2 kod eski modelin temizlenmesi icin gerekli\n",
    "    # yine de tensorflow'dan kaynakli olarak, bazen silinmeyebilir ve gpu dolabilir,\n",
    "    # allocation ile ilgili bir hata alinirsa kernel bastan baslatilmali\n",
    "    del beauty_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeli kaydetmek istersek yorum satiri acilabilir\n",
    "#model.save('models/white-female-new.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[92.30769276618958, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0, 100.0]\n",
      "[0.13040348887443542, 0.0005123050068505108, 1.595553953848139e-06, 0.0, 3.874292815453373e-07, 2.9802320611338473e-08, 0.0, 0.0]\n",
      "average acc:99.0384615957737\n",
      "average loss:0.016364725833355243\n"
     ]
    }
   ],
   "source": [
    "print(f'acc_per_fold: {acc_per_fold}')\n",
    "print(f'loss_per_fold: {loss_per_fold}')\n",
    "print(f'average acc:{sum(acc_per_fold)/len(acc_per_fold)}')\n",
    "print(f'average loss:{sum(loss_per_fold)/len(loss_per_fold)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[49  1]\n",
      " [ 0 50]]\n",
      "           Beautiful  Normal\n",
      "Beautiful         49       1\n",
      "Normal             0      50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1aa62a18250>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(sum(cm_per_fold))\n",
    "df_cm = pd.DataFrame(sum(cm_per_fold), index=beauty_classes, columns=beauty_classes)\n",
    "print(df_cm)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, fmt=\".0f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
