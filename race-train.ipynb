{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import dlib\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.face_utils import FaceAligner\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Activation\n",
    "from keras.layers import Conv2D, AveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\"\n",
    "tqdm.pandas()\n",
    "\n",
    "# fotograflarin getirilecegi boyut\n",
    "target_size = (224, 224)\n",
    "\n",
    "# yuz tespiti icin detektor\n",
    "face_detector = dlib.get_frontal_face_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fotografta yuz var mi kontrolu\n",
    "def check_detection(img):\n",
    "    exact_image = False\n",
    "    if type(img).__module__ == np.__name__:\n",
    "        exact_image = True\n",
    "\n",
    "    base64_img = False\n",
    "    if len(img) > 11 and img[0:11] == \"data:image/\":\n",
    "        base64_img = True\n",
    "\n",
    "    elif not exact_image:  # image path passed as input\n",
    "\n",
    "        if not os.path.isfile(img):\n",
    "            raise ValueError(\"Confirm that \", img, \" exists\")\n",
    "\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    rects, scores, idx = face_detector.run(gray,1,-1)\n",
    "            \n",
    "    if len(rects) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# yuz tespit eden fonksyion\n",
    "def detect_face(img):\n",
    "    exact_image = False\n",
    "    if type(img).__module__ == np.__name__:\n",
    "        exact_image = True\n",
    "\n",
    "    base64_img = False\n",
    "    if len(img) > 11 and img[0:11] == \"data:image/\":\n",
    "        base64_img = True\n",
    "\n",
    "    elif not exact_image:  # image path passed as input\n",
    "        if not os.path.isfile(img):\n",
    "            raise ValueError(\"Confirm that \", img, \" exists\")\n",
    "\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # rects: yuz tespiti yapilan cerceve\n",
    "    # scores: yuz tespiti skorlari\n",
    "    # best_score_index: en yuksek skora sahip yuzun indisi\n",
    "    rects, scores, idx = face_detector.run(gray,1,-1)\n",
    "    best_score_index = np.argmax(scores)\n",
    "    faces = []\n",
    "    # tespit edilen yuzler fotograftan kirpiliyor \n",
    "    for rect in rects:\n",
    "        detected_face = img[max(0, rect.top()): min(rect.bottom(), img.shape[0]),\n",
    "                    max(0, rect.left()): min(rect.right(), img.shape[1])]\n",
    "        \n",
    "        # fotograflar 224, 224 hale getiriliyor\n",
    "        detected_face = cv2.resize(detected_face, (target_size[0], target_size[1]))\n",
    "        faces.append(detected_face)\n",
    "\n",
    "    return faces[best_score_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veri okundu\n",
    "train_df = pd.read_csv(\"fairface/fairface_label_train.csv\")\n",
    "test_df = pd.read_csv(\"fairface/fairface_label_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train set: \",train_df.shape)\n",
    "print(\"test set: \",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[['file', 'race']]\n",
    "test_df = test_df[['file', 'race']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['file'] = 'fairface/'+train_df['file']\n",
    "test_df['file'] = 'fairface/'+test_df['file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veri setindeki Ä±rklara gore veri dagilimi\n",
    "100*train_df.groupby(['race']).count()[['file']]/train_df.groupby(['race']).count()[['file']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# East ve Southeast Asian, Asian olarak birlestiriliyor.\n",
    "idx = train_df[(train_df['race'] == 'East Asian') | (train_df['race'] == 'Southeast Asian')].index\n",
    "train_df.loc[idx, 'race'] = 'Asian'\n",
    "\n",
    "idx = test_df[(test_df['race'] == 'East Asian') | (test_df['race'] == 'Southeast Asian')].index\n",
    "test_df.loc[idx, 'race'] = 'Asian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "100*train_df.groupby(['race']).count()[['file']]/train_df.groupby(['race']).count()[['file']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yuz tespiti kontrolu\n",
    "train_df['detection'] = train_df['file'].progress_apply(check_detection)\n",
    "test_df['detection'] = test_df['file'].progress_apply(check_detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['detection'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yuz tespit edilemeyen fotograflar cikartiliyor\n",
    "train_df = train_df[train_df.detection == True]\n",
    "test_df = test_df[test_df.detection == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yuz tespiti\n",
    "train_df['pixels'] = train_df['file'].progress_apply(detect_face)\n",
    "test_df['pixels'] = test_df['file'].progress_apply(detect_face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = []\n",
    "test_features = []\n",
    "\n",
    "for i in range(0, train_df.shape[0]):\n",
    "    train_features.append(train_df['pixels'].values[i])\n",
    "\n",
    "for i in range(0, test_df.shape[0]):\n",
    "    test_features.append(test_df['pixels'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verilerin egitimde kullanilabilmesi icin uygun formata getiriliyor\n",
    "train_features = np.array(train_features)\n",
    "test_features = np.array(test_features)\n",
    "\n",
    "train_features = train_features / 255\n",
    "test_features = test_features / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_df[['race']]\n",
    "test_label = test_df[['race']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "races = train_df['race'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labellar categorical hale getiriliyor\n",
    "for j in range(len(races)):\n",
    "    current_race = races[j]\n",
    "    print(\"replacing \",current_race,\" to \", j+1)\n",
    "    train_label['race'] = train_label['race'].replace(current_race, str(j+1))\n",
    "    test_label['race'] = test_label['race'].replace(current_race, str(j+1))\n",
    "\n",
    "train_label = train_label.astype({'race': 'int32'})\n",
    "test_label = test_label.astype({'race': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.get_dummies(train_label['race'], prefix='race')\n",
    "test_target = pd.get_dummies(test_label['race'], prefix='race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(train_features, train_target.values\n",
    "                                        , test_size=0.12, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Activation\n",
    "from keras.layers import Conv2D, AveragePooling2D\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG-Face model\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg-face agirliklari yuklendi\n",
    "model.load_weights('vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# son 7 katman egitime acildi, ek katmanlar eklendi\n",
    "for layer in model.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "\n",
    "base_model_output = Sequential()\n",
    "base_model_output = Convolution2D(num_of_classes, (1, 1), name='predictions')(model.layers[-4].output)\n",
    "base_model_output = Flatten()(base_model_output)\n",
    "base_model_output = Activation('softmax')(base_model_output)\n",
    "\n",
    "race_model = Model(inputs=model.input, outputs=base_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_model.compile(loss='categorical_crossentropy'\n",
    "                  , optimizer=keras.optimizers.Adam()\n",
    "                  , metrics=['accuracy']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Egitim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /models klasoru olmasi gereklidir. Her tur sonunda en iyi model kontrolu yapilarak kaydedilecektir. \n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='race_model.hdf5'\n",
    "    , monitor = \"val_loss\"\n",
    "    , verbose=1\n",
    "    , save_best_only=True\n",
    "    , mode = 'auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# patience early stopping icin gerekli,\n",
    "# val_loss'un azalmasinin kac epoch beklenecegini belirtir. Azalmazsa egitimi durdurur.\n",
    "patience = 3\n",
    "epochs = 10\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=patience) \n",
    "\n",
    "score = race_model.fit(\n",
    "    train_x, train_y\n",
    "    , epochs=epochs\n",
    "    , validation_data=(val_x, val_y)\n",
    "    , callbacks=[checkpointer, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en iyi model yukleniyor\n",
    "from keras.models import load_model\n",
    "race_model = load_model(\"race_model.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_perf = race_model.evaluate(test_features, test_target.values, verbose=1)\n",
    "print(test_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_perf = race_model.evaluate(val_x, val_y, verbose=1)\n",
    "print(validation_perf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = race_model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_classes = []; actual_classes = []\n",
    "\n",
    "for i in range(0, predictions.shape[0]):\n",
    "    prediction = np.argmax(predictions[i])\n",
    "    prediction_classes.append(races[prediction])\n",
    "    actual = np.argmax(test_target.values[i])\n",
    "    actual_classes.append(races[actual])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(actual_classes, prediction_classes)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cm = pd.DataFrame(cm, index=races, columns=races)\n",
    "sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 10})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
