{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.face_utils import FaceAligner\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Activation\n",
    "from keras.layers import Conv2D, AveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "import scipy.io\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (224, 224)\n",
    "# yuz tespiti yapan detektor\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# fotografta yuz var mi kontrolu\n",
    "def check_detection(img):\n",
    "    exact_image = False\n",
    "    if type(img).__module__ == np.__name__:\n",
    "        exact_image = True\n",
    "\n",
    "    base64_img = False\n",
    "    if len(img) > 11 and img[0:11] == \"data:image/\":\n",
    "        base64_img = True\n",
    "\n",
    "\n",
    "    elif not exact_image:  # image path passed as input\n",
    "\n",
    "        if not os.path.isfile(img):\n",
    "            raise ValueError(\"Confirm that \", img, \" exists\")\n",
    "\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    rects, scores, idx = face_detector.run(gray,1,-1)\n",
    "            \n",
    "    if len(rects) > 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "# yuz tespit eden fonksyion\n",
    "def detect_face(img):\n",
    "    exact_image = False\n",
    "    if type(img).__module__ == np.__name__:\n",
    "        exact_image = True\n",
    "\n",
    "    base64_img = False\n",
    "    if len(img) > 11 and img[0:11] == \"data:image/\":\n",
    "        base64_img = True\n",
    "\n",
    "    elif not exact_image:\n",
    "        if not os.path.isfile(img):\n",
    "            raise ValueError(\"Confirm that \", img, \" exists\")\n",
    "\n",
    "        img = cv2.imread(img)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    # rects: yuz tespiti yapilan cerceve\n",
    "    # scores: yuz tespiti skorlari\n",
    "    # best_score_index: en yuksek skora sahip yuzun indisi\n",
    "    rects, scores, idx = face_detector.run(gray,1,-1)\n",
    "    best_score_index = np.argmax(scores)\n",
    "    faces = []\n",
    "    # tespit edilen yuzler fotograftan kirpiliyor \n",
    "    for rect in rects:\n",
    "        detected_face = img[max(0, rect.top()): min(rect.bottom(), img.shape[0]),\n",
    "                    max(0, rect.left()): min(rect.right(), img.shape[1])]\n",
    "        \n",
    "        # fotograflar 224, 224 hale getiriliyor\n",
    "        detected_face = cv2.resize(detected_face, (target_size[0], target_size[1]))\n",
    "        faces.append(detected_face)\n",
    "\n",
    "    return faces[best_score_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veri setinde yasi hesaplayan fonksiyon\n",
    "def datenum_to_datetime(datenum):\n",
    "    days = datenum % 1\n",
    "\n",
    "    hours = days % 1 * 24\n",
    "    minutes = hours % 1 * 60\n",
    "    seconds = minutes % 1 * 60\n",
    "    exact_date = datetime.fromordinal(int(datenum)) \\\n",
    "                 + timedelta(days=int(days)) + timedelta(hours=int(hours)) \\\n",
    "                 + timedelta(minutes=int(minutes)) + timedelta(seconds=round(seconds)) \\\n",
    "                 - timedelta(days=366)\n",
    "\n",
    "    return exact_date.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veriyi uygun formata getiren fonksiyon\n",
    "def get_pixels(img_path):\n",
    "    new_img_path = \"wiki_crop/\" + img_path[0]\n",
    "\n",
    "    img = load_img(new_img_path, grayscale=False, target_size=(224, 224))\n",
    "    img_pixels = img_to_array(img)\n",
    "    img_pixels = np.expand_dims(img_pixels, axis=0)\n",
    "    img_pixels /= 255\n",
    "    return img_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verinin okunmasi\n",
    "mat = scipy.io.loadmat('wiki_crop/wiki.mat')\n",
    "instances = mat['wiki'][0][0][0].shape[1]\n",
    "\n",
    "columns = [\"dob\", \"photo_taken\", \"full_path\", \"gender\", \"name\", \"face_location\", \"face_score\", \"second_face_score\"]\n",
    "df = pd.DataFrame(index=range(0, instances), columns=columns)\n",
    "\n",
    "for i in mat:\n",
    "    if i == \"wiki\":\n",
    "        current_array = mat[i][0][0]\n",
    "for j in range(len(current_array)):\n",
    "    df[columns[j]] = pd.DataFrame(current_array[j][0])\n",
    "\n",
    "df['date_of_birth'] = df['dob'].apply(datenum_to_datetime)\n",
    "df['age'] = df['photo_taken'] - df['date_of_birth']\n",
    "# yuzu olmayan fotograflar cikartiliyor\n",
    "df = df[df['face_score'] != -np.inf]\n",
    "\n",
    "# birden fazla yuz barindiran fotograflar cikartiliyor\n",
    "df = df[df['second_face_score'].isna()]\n",
    "\n",
    "# yuz skoru threshold altinda kalanlar cikartiliyor\n",
    "df = df[df['face_score'] >= 3]\n",
    "\n",
    "# cinsiyeti olmayan fotograflar cikartiliyor\n",
    "df = df[~df['gender'].isna()]\n",
    "\n",
    "df = df.drop(\n",
    "    columns=['name', 'face_score', 'second_face_score', 'date_of_birth', 'face_location', 'dob', 'photo_taken'])\n",
    "# 0 - 100 yas araliginda olmayanlar cikartiliyor\n",
    "df = df[df['age'] <= 100]\n",
    "df = df[df['age'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_path</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[17/10000217_1981-05-05_2009.jpg]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[12/100012_1948-07-03_2008.jpg]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[16/10002116_1971-05-31_2012.jpg]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[02/10002702_1960-11-09_2012.jpg]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[41/10003541_1937-09-27_1971.jpg]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           full_path  gender  age\n",
       "0  [17/10000217_1981-05-05_2009.jpg]     1.0   28\n",
       "2    [12/100012_1948-07-03_2008.jpg]     1.0   60\n",
       "4  [16/10002116_1971-05-31_2012.jpg]     0.0   41\n",
       "5  [02/10002702_1960-11-09_2012.jpg]     0.0   52\n",
       "6  [41/10003541_1937-09-27_1971.jpg]     1.0   34"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathlerin uygun formata getirilmesi\n",
    "paths = []\n",
    "for index, i in enumerate(df['full_path']):\n",
    "    paths.append(i[0])\n",
    "df['path'] = paths\n",
    "df['path'] = \"wiki_crop/\" + df['path']\n",
    "df = df.drop(columns=['full_path', 'age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>wiki_crop/17/10000217_1981-05-05_2009.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>wiki_crop/12/100012_1948-07-03_2008.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>wiki_crop/16/10002116_1971-05-31_2012.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>wiki_crop/02/10002702_1960-11-09_2012.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>wiki_crop/41/10003541_1937-09-27_1971.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   gender                                       path\n",
       "0     1.0  wiki_crop/17/10000217_1981-05-05_2009.jpg\n",
       "2     1.0    wiki_crop/12/100012_1948-07-03_2008.jpg\n",
       "4     0.0  wiki_crop/16/10002116_1971-05-31_2012.jpg\n",
       "5     0.0  wiki_crop/02/10002702_1960-11-09_2012.jpg\n",
       "6     1.0  wiki_crop/41/10003541_1937-09-27_1971.jpg"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yuz tespit edilemeyen fotograflarin cikartilmasi\n",
    "df['detection'] = df['path'].apply(check_detection)\n",
    "df = df[df.detection == True]\n",
    "df = df.drop(columns='detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verinin training icin uygun hale getirilmesi\n",
    "classes = 2\n",
    "target = df['gender'].values\n",
    "target_classes = keras.utils.to_categorical(target, classes)\n",
    "\n",
    "features = []\n",
    "\n",
    "for i in range(0, df.shape[0]):\n",
    "    features.append(df['path'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(features, target_classes, test_size=0.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# veri setinde yuz tespiti yapiliyor\n",
    "temp_list = []\n",
    "for i in train_x:\n",
    "    img = detect_face(i)\n",
    "    temp_list.append(img)\n",
    "\n",
    "train_x = temp_list\n",
    "temp_list2 = []\n",
    "    \n",
    "for i in test_x:\n",
    "    img = detect_face(i)\n",
    "    temp_list2.append(img)\n",
    "\n",
    "test_x = temp_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veriler egitim icin uygun formata getiriliyor (memory tuketimi fazla olabilir)\n",
    "train_x = np.array(train_x)/255\n",
    "test_x = np.array(test_x)/255\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y\n",
    "                                        , test_size=0.1, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vgg-face modeli\n",
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1, 1), input_shape=(224, 224, 3)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(ZeroPadding2D((1, 1)))\n",
    "model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "\n",
    "model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Convolution2D(2622, (1, 1)))\n",
    "model.add(Flatten())\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.load_weights('vgg_face_weights.h5')\n",
    "\n",
    "for layer in model.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# son katmanlar eklenerek egitim icin hazir hale getiriliyor\n",
    "base_model_output = Sequential()\n",
    "base_model_output = Convolution2D(classes, (1, 1), name='predictions')(model.layers[-4].output)\n",
    "base_model_output = Flatten()(base_model_output)\n",
    "base_model_output = Activation('softmax')(base_model_output)\n",
    "\n",
    "beauty_model = Model(inputs=model.input, outputs=base_model_output)\n",
    "\n",
    "beauty_model.compile(loss='categorical_crossentropy'\n",
    "                  , optimizer=keras.optimizers.Adam()\n",
    "                  , metrics=['accuracy']\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "498/498 [==============================] - ETA: 0s - loss: 0.1634 - accuracy: 0.9474\n",
      "Epoch 00001: val_loss improved from inf to 0.10526, saving model to models/gender_new.hdf5\n",
      "498/498 [==============================] - 219s 439ms/step - loss: 0.1634 - accuracy: 0.9474 - val_loss: 0.1053 - val_accuracy: 0.9610\n",
      "Epoch 2/10\n",
      "498/498 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9665\n",
      "Epoch 00002: val_loss did not improve from 0.10526\n",
      "498/498 [==============================] - 214s 429ms/step - loss: 0.1075 - accuracy: 0.9665 - val_loss: 0.1267 - val_accuracy: 0.9644\n",
      "Epoch 3/10\n",
      "498/498 [==============================] - ETA: 0s - loss: 0.0873 - accuracy: 0.9739\n",
      "Epoch 00003: val_loss improved from 0.10526 to 0.09210, saving model to models/gender_new.hdf5\n",
      "498/498 [==============================] - 221s 445ms/step - loss: 0.0873 - accuracy: 0.9739 - val_loss: 0.0921 - val_accuracy: 0.9718\n",
      "Epoch 4/10\n",
      "498/498 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9783\n",
      "Epoch 00004: val_loss did not improve from 0.09210\n",
      "498/498 [==============================] - 189s 379ms/step - loss: 0.0759 - accuracy: 0.9783 - val_loss: 0.1154 - val_accuracy: 0.9610\n",
      "Epoch 5/10\n",
      "498/498 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9827\n",
      "Epoch 00005: val_loss did not improve from 0.09210\n",
      "498/498 [==============================] - 186s 374ms/step - loss: 0.0617 - accuracy: 0.9827 - val_loss: 0.1031 - val_accuracy: 0.9734\n",
      "Epoch 6/10\n",
      "498/498 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9847\n",
      "Epoch 00006: val_loss did not improve from 0.09210\n",
      "498/498 [==============================] - 184s 370ms/step - loss: 0.0549 - accuracy: 0.9847 - val_loss: 0.1026 - val_accuracy: 0.9763\n",
      "Epoch 7/10\n",
      "498/498 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9857\n",
      "Epoch 00007: val_loss did not improve from 0.09210\n",
      "498/498 [==============================] - 205s 411ms/step - loss: 0.0518 - accuracy: 0.9857 - val_loss: 0.1388 - val_accuracy: 0.9689\n",
      "Epoch 8/10\n",
      "498/498 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9868\n",
      "Epoch 00008: val_loss did not improve from 0.09210\n",
      "498/498 [==============================] - 204s 410ms/step - loss: 0.0475 - accuracy: 0.9868 - val_loss: 0.1329 - val_accuracy: 0.9695\n"
     ]
    }
   ],
   "source": [
    "# /models klasoru olmasi gereklidir. Her tur sonunda en iyi model kontrolu yapilarak kaydedilecektir. \n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='models/gender.hdf5'\n",
    "    , monitor=\"val_loss\"\n",
    "    , verbose=1\n",
    "    , save_best_only=True\n",
    "    , mode='auto'\n",
    ")\n",
    "\n",
    "# patience early stopping icin gerekli,\n",
    "# val_loss'un azalmasinin kac epoch beklenecegini belirtir. Azalmazsa egitimi durdurur.\n",
    "patience = 5\n",
    "epochs = 10\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=patience)\n",
    "\n",
    "score = beauty_model.fit(\n",
    "        train_x, train_y\n",
    "        , epochs=epochs\n",
    "        , validation_data=(val_x, val_y)\n",
    "        , callbacks=[checkpointer, early_stop]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en iyi model tekrar yuleniyor\n",
    "from keras.models import load_model\n",
    "beauty_model = load_model(\"models/gender.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 36s 255ms/step - loss: 0.0883 - accuracy: 0.9668\n",
      "results:[0.08834103494882584, 0.9667796492576599]\n"
     ]
    }
   ],
   "source": [
    "results = beauty_model.evaluate(test_x, test_y, verbose=1)\n",
    "print(f'results:{results}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3117   32]\n",
      " [ 115 1161]]\n",
      "       Woman   Man\n",
      "Woman   3117    32\n",
      "Man      115  1161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1ed1e89c070>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = beauty_model.predict(test_x)\n",
    "prediction_classes = []\n",
    "actual_classes = []\n",
    "beauty_classes = ['Woman', 'Man']\n",
    "for i in range(0, predictions.shape[0]):\n",
    "    prediction = np.argmax(predictions[i])\n",
    "    prediction_classes.append(beauty_classes[prediction])\n",
    "    actual = np.argmax(test_y[i])\n",
    "    actual_classes.append(beauty_classes[actual])\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "cm = confusion_matrix(actual_classes, prediction_classes)\n",
    "print(cm)\n",
    "df_cm = pd.DataFrame(cm, index=beauty_classes, columns=beauty_classes)\n",
    "print(df_cm)\n",
    "sn.heatmap(df_cm, annot=True, annot_kws={\"size\": 10}, fmt=\".0f\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
